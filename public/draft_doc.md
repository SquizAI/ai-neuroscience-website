Beyond Scaling: Why AI Needs Neuroscience to Achieve True Intelligence
Introduction
Artificial intelligence is advancing at unprecedented speed, and this primarily driven by the dominant paradigm of “scaling”—more compute, more data, and more parameters, i.e., larger models. The allure of scaling lies in its simplicity: if we keep making models bigger, perhaps we'll eventually achieve human-like AI, or Artificial General Intelligence (AGI)—machines capable of human-level intelligence, creativity, adaptability, and generalization. Yet, impressive as these large language models are, important, theoretical questions remain unanswered. Is scaling alone truly sufficient for achieving genuine understanding, human-like creativity or, more profoundly, consciousness? In this article, I argue that scaling, despite its practical successes, is fundamentally limited in its ability to produce true AGI. Instead, AI researchers will need insights from contemporary neuroscience—particularly concepts like Karl Friston’s Free Energy Principle (FEP), Scott Kelso’s metastability, and Anil Seth’s embodied consciousness—offer essential guidance. These approaches reveal critical blind spots in our current AI trajectory, challenging the simplistic notion that bigger will always mean better, and offering a richer, more integrated path toward genuine intelligence and creativity.
Scaling Laws and the Limits of "Bigger is Better"
The current AI development paradigm is built largely upon scaling laws—predictions suggesting that continually enlarging models with more parameters, greater computing resources, and larger datasets, inevitably improves their intelligence. Yet, Stuart Russell, a top AI researcher at Berkeley, sharply critiques this approach, highlighting the absence of fundamental guiding principles underlying these massive models, often called "giant black boxes." Scaling is an empirical, rather than theoretical, strategy: it lacks a solid scientific basis guaranteeing progress toward AGI. This example is why have theory in addition to observation is so important! Practical limitations loom -- finite amounts of useful data and physical limits on computing capacity. More troublingly, Russell points out that even impressive breakthroughs—such as AlphaGo’s acclaimed successes—can mask underlying misunderstandings, creating illusions of intelligence without genuine comprehension. This raises serious doubts about whether scaling alone could lead to true AGI. If scaling fails to deliver on its promises, we risk not only stagnation but a potentially devastating "AI winter," leaving the field economically and scientifically stranded. 
Emergent Abilities and Unpredictability at Scale
Complicating the picture further, recent research highlights emergent abilities—skills spontaneously arising only after models surpass certain size thresholds. Here, however, is a similarity with the human brain: emergence. Wei et al., 2022 report that tasks like arithmetic, multi-step reasoning, and multilingual understanding appear abruptly at specific scales, defying simple predictions. These emergent properties initially seem to support scaling strategies: perhaps the path to AGI is simply discovering more and larger emergent transitions. However, this unpredictability underlines a critical vulnerability: emergent abilities are fundamentally uncertain, appearing without warning and lacking clear theoretical explanations. Without a deeper understanding of why these transitions happen, we remain unable to predict future breakthroughs reliably. Yann LeCun, Chief AI Officer at Meta, and winner of the Turing Award, makes this point repeatedly. Instead, we depend on trial and error, an inherently risky strategy for developing something as significant as AGI. Such unpredictability underscores the urgency of grounding AI in robust scientific principles. I argue that to reliably approach AGI, we must look beyond empirical scaling to insights drawn from modern cognitive and computational neuroscience, where human-like intelligence emerges from coherent principles of cognition, embodiment, and consciousness.
The Neuroscience of Creativity: Free Energy Principle and Active Inference
One influential framework is Karl Friston’s Free Energy Principle (FEP), a strong theoretical framework with an abundance of empirical evidence, describes (and explains!) the brain as an adaptive, nonlinear dynamical system continually minimizing uncertainty via active inference. Unlike the passive, data-driven pattern-recognition of scaled AI models, the embodied brain actively engages with its environment in an action-perception cycle, to predict and control sensory inputs. Our brains act like prediction machines, constantly guessing what will happen next and updating those guesses when reality doesn’t match. When faced with uncertainty, we either adjust our models/beliefs or take action to make reality fit our expectations.  This active inference framework offers precisely what current scaling approaches lack: an underlying principle guiding genuine understanding and adaptive intelligence. Another complimentary theory is Scott Kelso’s metastability, where the brain flexibly transitions between order and disorder, explains the brain's profound adaptability. Both of these theories rely on the notion of embodied cognition, which further illuminates the limitations of purely computational models. According to this view, cognition fundamentally arises from dynamic interactions between an organism’s brain, body, and environment. AI systems today are primarily disembodied, restricted to processing abstract symbols and detached data, profoundly limiting their capacity for real-world understanding. Without genuine embodiment—physical interactions that shape perception and cognition—AI models struggle to achieve true adaptability, insight, or genuine understanding. Embodied cognition researchers demonstrate that meaningful intelligence emerges only through real-time sensorimotor coupling with the environment. By incorporating embodiment into AI research, we move closer to replicating the adaptive and predictive qualities fundamental to biological intelligence. These neuroscience principles, centered around the self-organized interaction of mind, matter, and movement, highlight how true intelligence involves continuous adaptive interactions with the environment, rather than static pattern-matching alone. By incorporating FEP and metastability into AI development, we could guide models toward genuine cognition, moving beyond simplistic scaling paradigms and closer to true AGI.  
A Few Words On Prediction 
At first glance, AI models like ChatGPT and the human brain share a core similarity: both function as prediction engines. Both rely on patterns to make predictions - LLMs use text-based patterns, while the brain integrates sensory and lived experience patterns. However, the way they generate predictions reveals a profound difference in intelligence itself. Foundation models operate by statistically predicting the next word in a sequence, drawing from massive datasets to determine the most probable response. Right now, they probably lack true understanding, functioning instead as sophisticated pattern-matching machines that approximate coherence without genuine meaning, so only syntax, not semantics. In contrast, the human brain’s predictive engine, again the Free Energy Principle and active inference, works dynamically—constantly generating hypotheses about the world, updating beliefs through sensory experience, and adjusting behavior accordingly. The brain doesn't just predict passively; it acts to shape its environment in order to reduce uncertainty. This is true agency, a self-correcting loop of action-perception that is absent in today’s AI systems, which remain passive processors of text. This is the distinction between agentic AI, and agency. Without embodiment and an intrinsic drive to minimize uncertainty in real-world interaction, AI remains fundamentally limited—it can predict, but it cannot understand. The philosopher of information Luciano Floridi’s wrote a recent paper, AI as Agency Without Intelligence, which reinforces the fundamental distinction between LLM-based AI and human cognition. While LLMs display remarkable linguistic fluency, and may display a primitive form of agency, they operate as statistical pattern processors, not true intelligence systems. Floridi refines the popular "stochastic parrot" critique, noting that LLMs don’t simply regurgitate text—they synthesize and restructure data in novel, emergent ways, similar to a student stitching together an essay from multiple sources without deep comprehension. However, unlike the brain, which actively minimizes uncertainty through embodied interactions, LLMs passively predict text without understanding or goal-directed adaptation. Floridi’s critique underscores a key limitation of scaling approaches: without grounding in cognitive science and principles like active inference, AI risks remaining an impressive but fundamentally shallow imitation of intelligence rather than an authentic step toward AGI.
Consciousness vs. Intelligence: Insights from Anil Seth
Artificial general intelligence doesn’t imply acritical consciousness—a critical distinction underscored by neuroscientist Anil Seth. Intelligence involves flexible, goal-directed behavior, while consciousness involves subjective experiences and sensations—the vivid internal world that characterizes human experience. Contrary to common assumptions within the AI community, consciousness isn't merely algorithmic complexity running on the brain’s biological wetware. Instead, consciousness (may) emerge from being a living, embodied self-organzied organism fundamentally motivated by self-preservation. This perspective challenges the assumption that consciousness will spontaneously emerge from “simply” increasing intelligence. Even if AI were to reach human-level intelligence, consciousness might remain elusive unless explicitly accounted for. Thus, the distinction between intelligence and consciousness further emphasizes the necessity of neuroscientific frameworks. Understanding human consciousness through biological embodiment may be essential for moving beyond superficial, performance-driven AI toward something resembling genuine consciousness.
Agentic AI, Flow States, and Human-AI Synergy
Given this reasoned critique, but still optimistic about AI extending human cognition, especially creativity and intuition, we can bridge these neuroscience insights with practical agentic AI—autonomous systems capable of collaborative interaction with humans. Recent neuroscience research from Kotler et al., 2025, highlights how flow states - optimal conditions of peak performance and effortless creativity- integrate System 1 (fast, intuitive) and System 2 (deliberative, controlled) cognition, enabling seamless, adaptive high performance decision-making. Today’s AI almost mimics both—LLMs excel at quick pattern-based recognition (System 1), while inference-time computation enables multi-step reasoning (System 2). However, AI lacks embodied, dynamic interplay between these processes. Unlike human cognition, which fluidly integrates intuition and cognitive control, AI remains reactive and disembodied.
Agentic AI systems, guided by neuroscientific principles like active inference and embodied cognition, could seamlessly partner with humans, mutually enhancing creativity, intuition, performance, and problem-solving capabilities. An exciting possibility is integrating human flow states—optimal conditions of peak performance and effortless creativity—with AI assistance, resulting in continuous states of creative synergy. Future agentic AI must align with human cognitive rhythms, supporting flow states, enabling true human-AI synergy through active inference, adaptive control, and predictive, embodied intelligence. By grounding these agentic systems in neuroscience-derived principles, we transform AI from passive computational tools into true creative partners. Ultimately, this synthesis promises transformative advancements, enabling AI to augment rather than merely replicate human intelligence. Moving forward, embracing neuroscience in AI development is essential for responsibly navigating the pathway toward genuinely intelligent—and perhaps consciously aware—machines.


